{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13236811",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package imports and environment sanity check\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import chi2\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The bic value is computed using the deviance formula\"\n",
    ")\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc687c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display settings for pandas\n",
    "pd.set_option('display.width', 200)          # total characters per line before wrapping\n",
    "pd.set_option('display.max_columns', None)   # show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # don’t truncate long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406db574",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data and read data\n",
    "df_hist = pd.read_excel('data/Interview_data_2020_2024.xlsx')\n",
    "df_2025 = pd.read_excel('data/Interview_data_2025.xlsx')\n",
    "\n",
    "df_hist['frequency'] = df_hist['claims_number'] / df_hist['exposure']\n",
    "df_hist['dwelling_type'] = df_hist['dwelling_type'].astype('str')\n",
    "df_hist['dwelling_type'] = df_hist['dwelling_type'].astype('str')\n",
    "\n",
    "df_2025['dwelling_type'] = df_hist['building_class'].astype('str')\n",
    "df_2025['dwelling_type'] = df_hist['dwelling_type'].astype('str')\n",
    "\n",
    "df_all = pd.concat([df_hist, df_2025], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab4f88",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51da99",
   "metadata": {},
   "source": [
    "### Structural checks and data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f96572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datafframe sample:')\n",
    "display(df_all.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for  shape, column dtypes, memory usage and non-null counts\n",
    "print('Dataframe formats information:\\n')\n",
    "display(df_all.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b44975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe number of uniques:\")\n",
    "df_all.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of duplicated rows:', df_all.duplicated().sum())\n",
    "print('Number of negative exposures in:', (df_all['exposure'] < 0).sum())\n",
    "\n",
    "if df_all.duplicated().sum() > 0:\n",
    "    print('Duplicated rows in the dataframe:')\n",
    "elif (df_all['exposure']<0).sum() > 0:\n",
    "    print('Rows with negative exposure values:\\n', df_hist[df_hist['exposure'] < 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616c379",
   "metadata": {},
   "source": [
    "### Distribution checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe features metrics description:')\n",
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Claims number, Exposure, and Frequency distributions:\\n')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 4))\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.histplot(df_hist['claims_number'], bins=30, kde=False, ax=axes[0]).set_title('Claims Number Distribution')\n",
    "sns.histplot(df_hist['exposure'], bins=30, kde=False, ax=axes[1]).set_title('Exposure Distribution')\n",
    "sns.histplot(df_hist['frequency'], bins=30, kde=False, ax=axes[2]).set_title('Frequency Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664058d",
   "metadata": {},
   "source": [
    "### Relationship checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_hist.corr(numeric_only=True)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, cbar=True, annot_kws={\"size\": 7})\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d4c80",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Almost no correlation between factors which offers flexibility for feature selections\n",
    "- Weak but positive correlation between \n",
    "    - area and claims_number (0.08);   \n",
    "    - exposure and year (0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6985b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum exposure by each level\n",
    "exp_building = df_hist.groupby(\"building_class\")[\"exposure\"].sum().reset_index()\n",
    "exp_building[\"Variable\"] = \"Building Class\"\n",
    "exp_building.rename(columns={\"building_class\": \"Level\"}, inplace=True)\n",
    "\n",
    "exp_dwelling = df_hist.groupby(\"dwelling_type\")[\"exposure\"].sum().reset_index()\n",
    "exp_dwelling[\"Variable\"] = \"Dwelling Type\"\n",
    "exp_dwelling.rename(columns={\"dwelling_type\": \"Level\"}, inplace=True)\n",
    "\n",
    "bins = [0, 500, 1000, 2000, 30000]\n",
    "labels = [f\"[{bins[i]},{bins[i+1]})\" for i in range(len(bins)-1)]\n",
    "df_hist[\"area_band\"] = pd.cut(df_hist[\"area\"], bins=bins, right=False, labels=labels, include_lowest=True)\n",
    "\n",
    "exp_area = df_hist.groupby(\"area_band\")[\"exposure\"].sum().reset_index()\n",
    "exp_area[\"Variable\"] = \"Area Band\"\n",
    "exp_area.rename(columns={\"area_band\": \"Level\"}, inplace=True)\n",
    "\n",
    "# Combine all into one dataframe\n",
    "df_exp_all = pd.concat([exp_building, exp_dwelling, exp_area], ignore_index=True)\n",
    "\n",
    "# Plot all together\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df_exp_all, x=\"Level\", y=\"exposure\", hue=\"Variable\", estimator=sum)\n",
    "plt.title(\"Exposure by level across all variables\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7a7c9",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "We can already expect base levels per risk factor (if selected) to be:  \n",
    "- Building Class: 1  \n",
    "- Dwelling type: HR \n",
    "- Area band: [1000, 2000). Area bands are defined by judgement and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Claims number, exposure and frequency per year:\")\n",
    "\n",
    "g = (\n",
    "    df_hist.groupby('year', as_index=True)\n",
    "           .agg(claims_number=('claims_number','sum'),\n",
    "                exposures=('exposure','sum'))\n",
    "           .sort_index()\n",
    ")\n",
    "\n",
    "g['frequency'] = g['claims_number'] / g['exposures'].replace(0, np.nan)\n",
    "\n",
    "for col in ['claims_number', 'exposures', 'frequency']:\n",
    "    g[f'{col}_yoy'] = (g[col].pct_change() * 100).round(1)\n",
    "\n",
    "print(\n",
    "    g.round({\n",
    "        'claims_number': 0,\n",
    "        'exposures': 1,\n",
    "        'frequency': 6,          # keep precision; adjust as needed\n",
    "        'claims_number_yoy': 1,\n",
    "        'exposures_yoy': 1,\n",
    "        'frequency_yoy': 1\n",
    "    })\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 4), constrained_layout=True)\n",
    "\n",
    "sns.lineplot(data=g.reset_index(), x=\"year\", y=\"claims_number\", marker=\"o\", ax=axes[0])\n",
    "axes[0].set_title('Claims by year')\n",
    "\n",
    "sns.lineplot(data=g.reset_index(), x=\"year\", y=\"exposures\", marker=\"o\", ax=axes[1])\n",
    "axes[1].set_title('Exposure by year')\n",
    "\n",
    "sns.lineplot(data=g.reset_index(), x=\"year\", y=\"frequency\", marker=\"o\", ax=axes[2])\n",
    "axes[2].set_title('Frequency by year')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(sorted(g.reset_index()['year'].unique()))  # one tick per year\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25db8bc",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Between 2020 and 2024, exposure increases steadily yoy while claim counts drops in 2021 (+12.5%) and peaks again in 2022 (+28.8%) vs ~17% average growth.\n",
    "- Similarly to claims_number, increasing frequency between 2020 and 2024 but smoother pattern. Recent increase of +3.1% (as per average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ff95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequency by risk factor 'building_class'\")\n",
    "\n",
    "freq_by_class = (\n",
    "    df_hist.groupby(['building_class','year'], as_index=True)\n",
    "           .agg(claims=('claims_number','sum'),\n",
    "                exp=('exposure','sum'))\n",
    "           .sort_index()\n",
    ")\n",
    "\n",
    "freq_by_class['freq'] = freq_by_class['claims'] / freq_by_class['exp'].replace(0, np.nan)\n",
    "\n",
    "for col in ['claims', 'exp', 'freq']:\n",
    "    freq_by_class[f'{col}_yoy'] = (freq_by_class.groupby(level=0)[col].pct_change() * 100).round(1)\n",
    "\n",
    "print('freq_by_class with YoY changes:\\n', freq_by_class.round(3))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 4), constrained_layout=True)\n",
    "sns.lineplot(data=freq_by_class.reset_index(), x=\"year\", y=\"claims\", hue=\"building_class\", marker=\"o\", ax=axes[0])\n",
    "axes[0].set_title('Claims by building_class and year')\n",
    "\n",
    "sns.lineplot(data=freq_by_class.reset_index(), x=\"year\", y=\"exp\", hue=\"building_class\", marker=\"o\", ax=axes[1])\n",
    "axes[1].set_title('Exposure by building_class and year')\n",
    "\n",
    "sns.lineplot(data=freq_by_class.reset_index(), x=\"year\", y=\"freq\", hue=\"building_class\", marker=\"o\", ax=axes[2])\n",
    "axes[2].set_title('Frequency by building_class and year')\n",
    "\n",
    "years = sorted(df_hist['year'].unique())\n",
    "for ax in axes:\n",
    "    ax.set_xticks(years)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    leg = ax.get_legend()\n",
    "    if leg is not None:\n",
    "        leg.set_title('Building class')\n",
    "        leg.set_frame_on(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fddbf",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Analyzing growth by building class, exposure again increases steadily between 2020 and 2024. Claim counts again spikes in 2022 with +29.4% (Class 1), +32.4% (Class 2), and +24.0% (Class 3). Due to a slower incresaing exposure, frequency is less volatile than claims counts, but similarly increasing:\n",
    "- Class 1 frequency (highest exposure) is second most stable: Steady, moderate increases but converging (3.6% most recently vs. 3.7% average)\n",
    "- Class 2 frequency shows the most volatility: Massive spikes in claims (32.4% in 2022, 25.1% in 2024) with corresponding frequency jumps (17.3% in 2022, 13.1% in 2024 vs. ~+7% average)\n",
    "- Class 3 frequency is most stable and recently improved: 2024 saw -7.3% yoy (vs. ~+1.2% average) despite continued exposure growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequency by risk factor 'dwelling_type'\")\n",
    "\n",
    "freq_by_dwelling_type = (\n",
    "    df_hist.groupby(['dwelling_type','year'], as_index=True)\n",
    "           .agg(claims=('claims_number','sum'),\n",
    "                exp=('exposure','sum'))\n",
    "           .sort_index()\n",
    ")\n",
    "\n",
    "freq_by_dwelling_type['freq'] = freq_by_dwelling_type['claims'] / freq_by_dwelling_type['exp'].replace(0, np.nan)\n",
    "\n",
    "for col in ['claims', 'exp', 'freq']:\n",
    "    freq_by_dwelling_type[f'{col}_yoy'] = (freq_by_dwelling_type.groupby(level=0)[col].pct_change() * 100).round(1)\n",
    "\n",
    "print('freq_by_dwelling_type with YoY changes:\\n', freq_by_dwelling_type.round(3))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 4), constrained_layout=True)\n",
    "sns.lineplot(data=freq_by_dwelling_type.reset_index(), x=\"year\", y=\"claims\", hue=\"dwelling_type\", marker=\"o\", ax=axes[0])\n",
    "axes[0].set_title('Claims by dwelling_type and year')\n",
    "\n",
    "sns.lineplot(data=freq_by_dwelling_type.reset_index(), x=\"year\", y=\"exp\", hue=\"dwelling_type\", marker=\"o\", ax=axes[1])\n",
    "axes[1].set_title('Exposure by dwelling_type and year')\n",
    "\n",
    "sns.lineplot(data=freq_by_dwelling_type.reset_index(), x=\"year\", y=\"freq\", hue=\"dwelling_type\", marker=\"o\", ax=axes[2])\n",
    "axes[2].set_title('Frequency by dwelling_type and year')\n",
    "\n",
    "years = sorted(df_hist['year'].unique())\n",
    "for ax in axes:\n",
    "    ax.set_xticks(years)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    leg = ax.get_legend()\n",
    "    if leg is not None:\n",
    "        leg.set_title('Dwelling type')\n",
    "        leg.set_frame_on(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c865c5e",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Whilst exposure grows steadily btw. 2020 and 2024, claims number grow similarly except for the 2022 spike of +29.0% (dt=BR) and +28.5% (dt=HR).\n",
    "- BR: Consistently higher baseline frequency (~0.035-0.041), experienced sharp 2021 drop (-8.1%) and 2022 spike (+14.4%).\n",
    "- HR (Highest exposure): Lower baseline frequency (~0.022-0.026), experienced sharp 2022 spike (+13.8%).\n",
    "- Recent trend: Both types converging - BR accelerating (+4.3% freq in 2024 vs 3.8% average) while HR stabilizing (+2.0% vs 4.1% average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequency by risk factor 'area_band'\")\n",
    "\n",
    "freq_by_area_band = (\n",
    "    df_hist.groupby(['area_band','year'], as_index=True)\n",
    "           .agg(claims=('claims_number','sum'),\n",
    "                exp=('exposure','sum'))\n",
    "           .sort_index()\n",
    ")\n",
    "\n",
    "freq_by_area_band['freq'] = freq_by_area_band['claims'] / freq_by_area_band['exp'].replace(0, np.nan)\n",
    "\n",
    "for col in ['claims', 'exp', 'freq']:\n",
    "    freq_by_area_band[f'{col}_yoy'] = (freq_by_area_band.groupby(level=0)[col].pct_change() * 100).round(1)\n",
    "\n",
    "print('freq_by_area_band with YoY changes:\\n', freq_by_area_band.round(3))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 4), constrained_layout=True)\n",
    "sns.lineplot(data=freq_by_area_band.reset_index(), x=\"year\", y=\"claims\", hue=\"area_band\", marker=\"o\", ax=axes[0])\n",
    "axes[0].set_title('Claims by area_band and year')\n",
    "\n",
    "sns.lineplot(data=freq_by_area_band.reset_index(), x=\"year\", y=\"exp\", hue=\"area_band\", marker=\"o\", ax=axes[1])\n",
    "axes[1].set_title('Exposure by area_band and year')\n",
    "\n",
    "sns.lineplot(data=freq_by_area_band.reset_index(), x=\"year\", y=\"freq\", hue=\"area_band\", marker=\"o\", ax=axes[2])\n",
    "axes[2].set_title('Frequency by area_band and year')\n",
    "\n",
    "years = sorted(df_hist['year'].unique())\n",
    "for ax in axes:\n",
    "    ax.set_xticks(years)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    leg = ax.get_legend()\n",
    "    if leg is not None:\n",
    "        leg.set_title('Area band')\n",
    "        leg.set_frame_on(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b407dcf",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Greater area_bands lead for higher frequencies and thus highest frequency in area band [2000, 30000).\n",
    "- Larger properties ([2000,30000)): \n",
    "    - Highest baseline frequency (0.054-0.060)\n",
    "    - Relatively stable growth pattern and improving in 2024 (-3.3% freq in 2024 vs +2.2% average)\n",
    "- ([1000,2000)) (Highest exposure):\n",
    "    - Moderate frequency (0.025-0.034)\n",
    "    - Accelerating upward trend (+6.8% freq in 2024 vs 4.2% average)\n",
    "- Small properties ([0,500) and [500,1000)):\n",
    "    - Lowest frequency historically\n",
    "    - BUT showing extreme volatility in recent years\n",
    "        - [0,500) band actually declined in 2024 (-13.1% frequency vs 3.3% average)\n",
    "        - [500,1000) spiked massively (+27.2% claims, +15.6% frequency in 2024 vs 8.75% average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84905b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers\n",
    "outliers = df_hist[df_hist['frequency'] > 2]\n",
    "print('Number of policies with freq > 2:', len(outliers))\n",
    "print(outliers[['policy_id','year','building_class','dwelling_type','exposure','claims_number','frequency']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d845b7",
   "metadata": {},
   "source": [
    "### Insight 1\n",
    "Area Band Shows the Clearest Pattern  \n",
    "1. Widest Range of Risk Profiles  \n",
    "    The area bands show a 6-7x difference in baseline frequency:  \n",
    "    - Small properties ([0,500)): ~0.009-0.011 frequency  \n",
    "    - Large properties ([2000,30000)): ~0.054-0.060 frequency  \n",
    "\n",
    "    Compared to:  \n",
    "    - Building class: Only ~1.5x difference (0.025 to 0.037)  \n",
    "    - Dwelling type: Only ~1.6x difference (0.022 to 0.041)  \n",
    "\n",
    "    -> This wider spread makes it easier to see distinct behavioral patterns across segments  \n",
    "\n",
    "2. More Distinct Behavioral Patterns  \n",
    "    Each area band tells a different story:  \n",
    "    [0,500): only 55 claims in 2024\n",
    "    [500,1000): Explosive recent growth - emerging situation\n",
    "    [1000,2000): Steady, predictable increases - the \"stable increasing middle\"  \n",
    "    [2000,30000): High but stable frequency - well-understood risk  \n",
    "\n",
    "    -> Versus building class/dwelling type where patterns are more similar across segments and baseline levels are converging.\n",
    "\n",
    "3. Clearer Divergence in Recent Years  \n",
    "    2024 frequency changes:  \n",
    "    [0,500): -13.1%  \n",
    "    [500,1000): +15.6%\n",
    "    [1000,2000): +6.8%  \n",
    "    [2000,30000): -3.3%  \n",
    "\n",
    "    -> This shows four completely different trajectories, suggesting area/value is a stronger predictor of claims behavior than the other variables \n",
    "\n",
    "4. Trend  \n",
    "    Given the overall upward trend, the year variable should be considered as an aditional explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f9ed2",
   "metadata": {},
   "source": [
    "### Temporal stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Portofolio Mix Shift Analysis\n",
    "# Count policies by year, class, dwelling\n",
    "counts = (\n",
    "    df_hist.groupby(['year','building_class', 'dwelling_type', 'area_band'])\n",
    "           .size()\n",
    "           .rename('policies')\n",
    ")\n",
    "\n",
    "# Normalize within each year → share of total\n",
    "mix_shift = counts / counts.groupby(level='year').transform('sum')\n",
    "mix_shift = mix_shift.reset_index()\n",
    "\n",
    "mix_pivot = mix_shift.pivot_table(\n",
    "    index='year', \n",
    "    columns=['building_class','dwelling_type', 'area_band'],\n",
    "    values='policies'\n",
    ")\n",
    "\n",
    "mix_pivot.plot(kind='bar', stacked=True, figsize=(10,6), colormap='tab20')\n",
    "plt.title('Portfolio Mix Shift by Year (Building Class × Dwelling Type × Area Band)', fontsize=14)\n",
    "plt.ylabel('Share of Portfolio')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title='Class × Type', bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545d520",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers \n",
    "def add_area_band(df):\n",
    "    return pd.cut(df['area'], bins=bins, right=False)\n",
    "\n",
    "def _one_hot(df, cols):\n",
    "    df_ = df.copy()\n",
    "    for c in cols:\n",
    "        df_[c] = df_[c].astype('category')\n",
    "    return pd.get_dummies(df_[cols], drop_first=True, dtype=float)\n",
    "\n",
    "def _interaction_cols(A: pd.DataFrame, B: pd.DataFrame, prefix='I_'):\n",
    "    out = {}\n",
    "    for a in A.columns:\n",
    "        for b in B.columns:\n",
    "            out[f'{prefix}{a}:@:{b}'] = A[a].values * B[b].values\n",
    "    return pd.DataFrame(out, index=A.index) if out else pd.DataFrame(index=A.index)\n",
    "\n",
    "def _three_way_cols(A: pd.DataFrame, B: pd.DataFrame, C: pd.DataFrame, prefix='I_'):\n",
    "    out = {}\n",
    "    for a in A.columns:\n",
    "        for b in B.columns:\n",
    "            for c in C.columns:\n",
    "                out[f'{prefix}{a}:@:{b}:@:{c}'] = A[a].values * B[b].values * C[c].values\n",
    "    return pd.DataFrame(out, index=A.index) if out else pd.DataFrame(index=A.index)\n",
    "\n",
    "def make_X(df, use_area_band: bool, include_bc: bool, include_dw: bool, interactions: set):\n",
    "    '''\n",
    "    interactions ∈ {'bcXdw','bcXarea','dwXarea','bcXdwXarea'}\n",
    "    Only applied if parent main effects are present.\n",
    "    '''\n",
    "    df_c = df.copy()\n",
    "\n",
    "    # Main effects (optional)\n",
    "    Z_parts = []\n",
    "\n",
    "    bc_block = pd.DataFrame(index=df_c.index)\n",
    "    dw_block = pd.DataFrame(index=df_c.index)\n",
    "\n",
    "    if include_bc:\n",
    "        bc_block = _one_hot(df_c, ['building_class'])[\n",
    "            [c for c in _one_hot(df_c, ['building_class']).columns if c.startswith('building_class_')]\n",
    "        ]\n",
    "        Z_parts.append(bc_block)\n",
    "\n",
    "    if include_dw:\n",
    "        dw_block = _one_hot(df_c, ['dwelling_type'])[\n",
    "            [c for c in _one_hot(df_c, ['dwelling_type']).columns if c.startswith('dwelling_type_')]\n",
    "        ]\n",
    "        Z_parts.append(dw_block)\n",
    "\n",
    "    # Area (always include, linear OR banded)\n",
    "    if use_area_band:\n",
    "        area_block = pd.get_dummies(add_area_band(df_c), drop_first=True, prefix='area', dtype=float)\n",
    "        area_is_linear = False\n",
    "    else:\n",
    "        area_block = pd.DataFrame({'area': df_c['area'].astype(float)})\n",
    "        area_is_linear = True\n",
    "    Z_parts.append(area_block)\n",
    "\n",
    "    Z = pd.concat(Z_parts, axis=1) if Z_parts else area_block.copy()\n",
    "\n",
    "    # --- Interactions (guarded by parent presence) ---\n",
    "    # bc × dw\n",
    "    if 'bcXdw' in interactions and include_bc and include_dw:\n",
    "        Z = Z.join(_interaction_cols(bc_block, dw_block, prefix='I_'))\n",
    "\n",
    "    # bc × area\n",
    "    if 'bcXarea' in interactions and include_bc:\n",
    "        if area_is_linear:\n",
    "            inter = {f'I_{b}:@:area': bc_block[b].values * area_block['area'].values for b in bc_block.columns}\n",
    "            if inter: Z = Z.join(pd.DataFrame(inter, index=Z.index))\n",
    "        else:\n",
    "            Z = Z.join(_interaction_cols(bc_block, area_block, prefix='I_'))\n",
    "\n",
    "    # dw × area\n",
    "    if 'dwXarea' in interactions and include_dw:\n",
    "        if area_is_linear:\n",
    "            inter = {f'I_{d}:@:area': dw_block[d].values * area_block['area'].values for d in dw_block.columns}\n",
    "            if inter: Z = Z.join(pd.DataFrame(inter, index=Z.index))\n",
    "        else:\n",
    "            Z = Z.join(_interaction_cols(dw_block, area_block, prefix='I_'))\n",
    "\n",
    "    # three-way bc × dw × area\n",
    "    if 'bcXdwXarea' in interactions and include_bc and include_dw:\n",
    "        if area_is_linear:\n",
    "            pair = _interaction_cols(bc_block, dw_block, prefix='I_')\n",
    "            for col in pair.columns:\n",
    "                pair[col+':@:area'] = pair[col].values * area_block['area'].values\n",
    "            keep = [c for c in pair.columns if c.endswith(':@:area')]\n",
    "            if keep: Z = Z.join(pair[keep])\n",
    "        else:\n",
    "            Z = Z.join(_three_way_cols(bc_block, dw_block, area_block, prefix='I_'))\n",
    "\n",
    "    # Constant & dtype\n",
    "    Z = sm.add_constant(Z, has_constant='add')\n",
    "    return Z.astype(float).reset_index(drop=True)\n",
    "\n",
    "# --- Time trend helper (numeric time instead of year dummies) ---\n",
    "def add_time_trend(X: pd.DataFrame, df: pd.DataFrame, *,\n",
    "                   base_year: int = None,\n",
    "                   include_t: bool = True,\n",
    "                   include_areaXt: bool = True,\n",
    "                   area_banded: bool = True):\n",
    "    \"\"\"\n",
    "    Adds a numeric time slope t = year - base_year to the design matrix X.\n",
    "    Optionally adds area × t interaction (banded or linear).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : design matrix from make_X (already has const, mains, and non-time interactions)\n",
    "    df : same rows as X, must contain 'year' and (if area_banded=False) 'area'\n",
    "    base_year : anchor; if None uses df['year'].min()\n",
    "    include_t : include plain time slope (t)\n",
    "    include_areaXt : include area × time slope\n",
    "    area_banded : True if area is represented by dummies in X, False if linear 'area' column\n",
    "    \"\"\"\n",
    "    if 'year' not in df.columns:\n",
    "        raise ValueError(\"add_time_trend: df must have a 'year' column.\")\n",
    "\n",
    "    by = int(df['year'].min()) if base_year is None else int(base_year)\n",
    "    t = (df['year'] - by).astype(float)\n",
    "\n",
    "    X = X.copy()\n",
    "    if include_t and ('t' not in X.columns):\n",
    "        X['t'] = t\n",
    "\n",
    "    if include_areaXt:\n",
    "        if area_banded:\n",
    "            area_cols = [c for c in X.columns if c.startswith('area_[')]\n",
    "            for c in area_cols:\n",
    "                X[f\"I_{c}:@:t\"] = X[c] * t\n",
    "        else:\n",
    "            if 'area' not in X.columns:\n",
    "                raise ValueError(\"add_time_trend: area_banded=False requires 'area' column in X.\")\n",
    "            X['I_area:@:t'] = X['area'] * t\n",
    "\n",
    "    return X, by\n",
    "\n",
    "#  Fit chooser & diagnostics\n",
    "def fit_glm_poisson_or_nb(y, X, exposure):\n",
    "    \"\"\"Fit Poisson; if overdispersed (>1.3), try NB and return chosen family, result, and Poisson dispersion.\"\"\"\n",
    "    # align and enforce numeric\n",
    "    y = y.astype(int).reset_index(drop=True)\n",
    "    X = X.astype(float).reset_index(drop=True)\n",
    "    exposure = exposure.reset_index(drop=True)\n",
    "\n",
    "    # guard against zero/neg exposures\n",
    "    if (exposure <= 0).any():\n",
    "        raise ValueError(\"Exposure must be strictly positive for log-offset.\")\n",
    "\n",
    "    # Poisson fit\n",
    "    mP = sm.GLM(y, X, family=sm.families.Poisson(), offset=np.log(exposure))\n",
    "    resP = mP.fit()\n",
    "    disp = float(resP.deviance / resP.df_resid)  # Poisson dispersion check\n",
    "\n",
    "    # If materially overdispersed, try NB\n",
    "    if disp > 1.3:\n",
    "        mNB = sm.GLM(y, X, family=sm.families.NegativeBinomial(), offset=np.log(exposure))\n",
    "        resNB = mNB.fit()\n",
    "        return (\"NB\", resNB, disp)\n",
    "\n",
    "    return (\"Poisson\", resP, disp)\n",
    "\n",
    "def normalized_gini(actual, pred):\n",
    "    \"\"\"Normalized (relative) Gini — robust & simple.\"\"\"\n",
    "    # Sort by prediction desc\n",
    "    order = np.argsort(-pred)\n",
    "    a = np.asarray(actual)[order]\n",
    "    # Lorenz area for model\n",
    "    cum_a = a.cumsum()\n",
    "    lor_model = cum_a / cum_a[-1] if cum_a[-1] != 0 else cum_a\n",
    "    x = np.arange(1, len(a)+1) / len(a)\n",
    "    g_model = np.trapezoid(lor_model, x) - 0.5\n",
    "    # Lorenz area for random (pred=constant) equals 0.5 baseline, so normalize vs perfect\n",
    "    # Compute \"perfect\" ordering by actual\n",
    "    order_star = np.argsort(-np.asarray(actual))\n",
    "    a_star = np.asarray(actual)[order_star]\n",
    "    cum_star = a_star.cumsum()\n",
    "    lor_star = cum_star / cum_star[-1] if cum_star[-1] != 0 else cum_star\n",
    "    g_star = np.trapezoid(lor_star, x) - 0.5\n",
    "    return (g_model / g_star) if g_star != 0 else 0.0\n",
    "\n",
    "def collect_metrics(res, y, mu, X):\n",
    "    \"\"\"Return a dict with AIC, BIC, dispersion, in-sample Poisson deviance, normalized Gini.\"\"\"\n",
    "    n = len(y)\n",
    "    k = len(res.params)  # includes intercept\n",
    "    # AIC direct from statsmodels; BIC: use res.bic if present else compute\n",
    "    aic = float(res.aic)\n",
    "    try:\n",
    "        bic = float(res.bic)\n",
    "    except Exception:\n",
    "        bic = (np.log(n) * k) - 2.0 * float(res.llf)\n",
    "\n",
    "    # Pearson chi-square / df (dispersion proxy)\n",
    "    # statsmodels provides res.pearson_chi2\n",
    "    pearson_over_df = float(res.pearson_chi2 / res.df_resid)\n",
    "\n",
    "    # Gini on expected counts (ranking power)\n",
    "    gini = normalized_gini(y, mu)\n",
    "\n",
    "    return {\n",
    "        \"AIC\": round(aic, 1),\n",
    "        \"BIC\": round(bic, 1),\n",
    "        \"dispersion_Poisson\": round(pearson_over_df, 3),\n",
    "        \"gini_normalized\": round(gini, 3),\n",
    "        \"k_parameters\": k\n",
    "    }\n",
    "\n",
    "def fmt(x, width=9, prec=1):\n",
    "    try:\n",
    "        return f\"{float(x):>{width}.{prec}f}\"\n",
    "    except Exception:\n",
    "        return f\"{str(x):>{width}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4aac5",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y   = df_hist['claims_number']\n",
    "exp = df_hist['exposure']\n",
    "\n",
    "# --- Main and interaction design grid ---\n",
    "main_effects_grid = [\n",
    "    (False, False),  # area only\n",
    "    (True,  False),  # bc only\n",
    "    (False, True),   # dw only\n",
    "    (True,  True),   # bc + dw\n",
    "]\n",
    "\n",
    "interaction_menu = [\n",
    "    set(), {'bcXdw'}, {'bcXarea'}, {'dwXarea'},\n",
    "    {'bcXdw','bcXarea'}, {'bcXdw','dwXarea'}, {'bcXarea','dwXarea'},\n",
    "    {'bcXdw','bcXarea','dwXarea'},\n",
    "    {'bcXdwXarea'}, {'bcXdw','bcXdwXarea'}, {'bcXarea','bcXdwXarea'},\n",
    "    {'dwXarea','bcXdwXarea'}, {'bcXdw','bcXarea','dwXarea','bcXdwXarea'},\n",
    "]\n",
    "\n",
    "MAX_PARAMS = 200\n",
    "candidates, model_num = [], 0\n",
    "\n",
    "print(colored(\"=== MODEL CANDIDATES (subsets + interactions) ===\", \"cyan\"))\n",
    "print(f\"{'No':<4} {'Spec':<60} {'Fam':<8} {'AIC':>9} {'BIC':>11} {'Pearson/df':>10} {'Gini':>6} {'k':>4}\")\n",
    "\n",
    "for use_area_band in [False, True]:\n",
    "    for include_bc, include_dw in main_effects_grid:\n",
    "        for inter_set in interaction_menu:\n",
    "\n",
    "            # Skip impossible parentless interactions\n",
    "            if ('bcXdw' in inter_set or 'bcXdwXarea' in inter_set) and not (include_bc and include_dw):\n",
    "                continue\n",
    "            if ('bcXarea' in inter_set) and not include_bc:\n",
    "                continue\n",
    "            if ('dwXarea' in inter_set) and not include_dw:\n",
    "                continue\n",
    "\n",
    "            X = make_X(df_hist, use_area_band, include_bc, include_dw, inter_set)\n",
    "            k = X.shape[1]\n",
    "            model_num += 1\n",
    "\n",
    "            spec_txt = (\n",
    "                f\"area={'banded' if use_area_band else 'linear'} | \"\n",
    "                f\"bc={'Y' if include_bc else 'N'} | \"\n",
    "                f\"dw={'Y' if include_dw else 'N'} | \"\n",
    "                f\"inter={'+'.join(sorted(inter_set)) or 'none'}\"\n",
    "            )\n",
    "\n",
    "            if k > MAX_PARAMS:\n",
    "                print(f\"{model_num:<4} {spec_txt[:58]:<60} {'SKIP':<8} {'—':>9} {'—':>11} {'—':>10} {'—':>6} {k:>4}\")\n",
    "                continue\n",
    "\n",
    "            fam, res, _ = fit_glm_poisson_or_nb(y, X, exp)\n",
    "            mu = res.predict(X, offset=np.log(exp))\n",
    "            m  = collect_metrics(res, y.values, mu, X)\n",
    "\n",
    "            candidates.append({\n",
    "                'spec': spec_txt,\n",
    "                'family': fam,\n",
    "                **m,\n",
    "                'k': k,\n",
    "                'res': res\n",
    "            })\n",
    "\n",
    "            print(f\"{model_num:<4} {(spec_txt[:58] + '…') if len(spec_txt) > 58 else spec_txt:<60} \"\n",
    "                  f\"{fam:<8} {fmt(m['AIC']):>9} {fmt(m['BIC']):>11} \"\n",
    "                  f\"{fmt(m['dispersion_Poisson'], prec=3):>10} \"\n",
    "                  f\"{fmt(m['gini_normalized'], prec=3):>6} {m['k_parameters']:>4}\")\n",
    "\n",
    "# --- Ranking and summary ---\n",
    "def rank_key(c):\n",
    "    area_is_banded = 'area=banded' in c['spec']\n",
    "    bcY = 'bc=Y' in c['spec']\n",
    "    dwY = 'dw=Y' in c['spec']\n",
    "    inter_token = c['spec'].split('inter=')[-1]\n",
    "    inter_count = 0 if inter_token == 'none' else inter_token.count('+') + 1\n",
    "    mains_count = int(bcY) + int(dwY)\n",
    "    return (c['AIC'], area_is_banded, mains_count, inter_count, c['k'])\n",
    "\n",
    "ranked = sorted(candidates, key=rank_key)\n",
    "choice = ranked[1]\n",
    "choice_res = choice['res']\n",
    "\n",
    "# --- Top 10 Table ---\n",
    "top15 = pd.DataFrame([\n",
    "    {\n",
    "        'spec': c['spec'],\n",
    "        'family': c['family'],\n",
    "        'AIC': round(c['AIC'], 1),\n",
    "        'BIC': round(c['BIC'], 1),\n",
    "        'Pearson/df': round(c['dispersion_Poisson'], 3),\n",
    "        'Gini': round(c['gini_normalized'], 3),\n",
    "        'k': c['k']\n",
    "    }\n",
    "    for c in ranked[:15]\n",
    "])\n",
    "\n",
    "print(\"\\n\" + colored(\"=== TOP 10 BEST MODELS (sorted by AIC) ===\", \"magenta\"))\n",
    "print(top15.to_string(index=False))\n",
    "\n",
    "# --- Chosen candidate summary ---\n",
    "print(\"\\n\" + colored(\"=== CHOSEN CANDIDATE MODEL ===\", \"green\"))\n",
    "print(f\"{choice['spec']} | Family: {choice['family']} | \"\n",
    "      f\"AIC={choice['AIC']:.1f} | BIC={choice['BIC']:.1f} | \"\n",
    "      f\"Pearson/df={choice['dispersion_Poisson']:.3f} | \"\n",
    "      f\"Gini={choice['gini_normalized']:.3f} | k={choice['k']}\")\n",
    "\n",
    "print(\"\\n\" + colored(\"=== FINAL MODEL SUMMARY ===\", \"yellow\"))\n",
    "print(choice_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878f9d5",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "a. Clear superiority of the banded area specification.\n",
    "Top 10 best-performing models (AIC ≈ 41 550–41 580) use area=banded.\n",
    "- This confirms that non-linearity in area (size segmentation) captures risk much better than treating area as a continuous variable.\n",
    "- Note that bins choice directly impacts this observation. Previously used bins should be considered.\n",
    "\n",
    "b. Importance of both building_class and dwelling_type.\n",
    "Top 10 candidates include both (bc=Y | dw=Y).\n",
    "- Dropping either leads to a ~120 AIC deterioration (vs. best candidate).\n",
    "- These categorical effects are essential.\n",
    "\n",
    "c. Small incremental gains from second-order interactions.\n",
    "Best model: inter=bcXdw+dwXarea.\n",
    "Next best: bcXdw alone (ΔAIC ≈ 0.4).\n",
    "- Adding both interactions only slightly improves AIC/BIC.\n",
    "- Third-order (bcXdwXarea) brings no gain and risks overfitting (>k).\n",
    "\n",
    "d. Good dispersion control.\n",
    "Pearson/df ≈ 1.00 across top models, means no strong over/under-dispersion. \n",
    "- Poisson family remains statistically valid and Negative Binomial not needed.\n",
    "\n",
    "e. Gini stability.\n",
    "Gini ≈ 0.33–0.34 is consistent across top models, confirming predictive ordering doesn’t change much. \n",
    "- Choice among top few specs won’t alter ranking behaviour.\n",
    "\n",
    "f. Coefficients significance.\n",
    "- 8 out of 9 terms are statistically significant.\n",
    "- The only non-significant effect is building_class_2, meaning only there’s no strong evidence that class 2 differs from the reference (class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825a3a8",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spec to validate\n",
    "USE_AREA_BANDED = True          # area=banded\n",
    "INCLUDE_BC = True               # bc=Y\n",
    "INCLUDE_DW = True               # dw=Y\n",
    "BASE_INTERACTIONS = {'bcXdw'}   # include bcXdw\n",
    "\n",
    "# Base design matrix\n",
    "y   = df_hist['claims_number']\n",
    "exp = df_hist['exposure']\n",
    "\n",
    "X_base = make_X(\n",
    "    df_hist,\n",
    "    use_area_band=USE_AREA_BANDED,\n",
    "    include_bc=INCLUDE_BC,\n",
    "    include_dw=INCLUDE_DW,\n",
    "    interactions=BASE_INTERACTIONS\n",
    ")\n",
    "if 'const' in X_base.columns:\n",
    "    X_base['const'] = 1.0\n",
    "\n",
    "# Fit base model (no time interaction)\n",
    "base_res = sm.GLM(y, X_base, family=sm.families.Poisson(), offset=np.log(exp)).fit()\n",
    "base_llf = base_res.llf\n",
    "base_aic = base_res.aic\n",
    "\n",
    "# Utilities to add time (year) interactions safely\n",
    "def _year_block(df):\n",
    "    return pd.get_dummies(df['year'].astype('category'),\n",
    "                          drop_first=True, prefix='year', dtype=float)\n",
    "\n",
    "def _select_cols(Z, prefix):\n",
    "    return [c for c in Z.columns if c.startswith(prefix)]\n",
    "\n",
    "def add_time_interactions(X_base, df, which={'bcXyear','dwXyear','areaXyear'}):\n",
    "    \"\"\"Return X_time = X_base + requested year interactions (guards on parent presence).\"\"\"\n",
    "    X_time = X_base.copy()\n",
    "    Yblk = _year_block(df)\n",
    "\n",
    "    bc_cols = _select_cols(X_base, 'building_class_')\n",
    "    dw_cols = _select_cols(X_base, 'dwelling_type_')\n",
    "    area_is_linear = ('area' in X_base.columns)\n",
    "    area_cols = _select_cols(X_base, 'area_') if not area_is_linear else []\n",
    "\n",
    "    # bc × year\n",
    "    if 'bcXyear' in which and bc_cols:\n",
    "        inter = {f\"I_{b}:@:{y}\": X_base[b].values * Yblk[y].values\n",
    "                 for b in bc_cols for y in Yblk.columns}\n",
    "        X_time = X_time.join(pd.DataFrame(inter, index=X_time.index))\n",
    "\n",
    "    # dw × year\n",
    "    if 'dwXyear' in which and dw_cols:\n",
    "        inter = {f\"I_{d}:@:{y}\": X_base[d].values * Yblk[y].values\n",
    "                 for d in dw_cols for y in Yblk.columns}\n",
    "        X_time = X_time.join(pd.DataFrame(inter, index=X_time.index))\n",
    "\n",
    "    # area × year\n",
    "    if 'areaXyear' in which:\n",
    "        if area_is_linear:\n",
    "            inter = {f\"I_area:@:{y}\": X_base['area'].values * Yblk[y].values\n",
    "                     for y in Yblk.columns}\n",
    "        else:\n",
    "            inter = {f\"I_{a}:@:{y}\": X_base[a].values * Yblk[y].values\n",
    "                     for a in area_cols for y in Yblk.columns}\n",
    "        X_time = X_time.join(pd.DataFrame(inter, index=X_time.index))\n",
    "\n",
    "    if 'const' in X_time.columns:\n",
    "        X_time['const'] = 1.0\n",
    "    return X_time\n",
    "\n",
    "# Fit time-interaction variants + LR Tests\n",
    "tests = []\n",
    "\n",
    "def fit_and_compare(X_new, label):\n",
    "    res_new = sm.GLM(y, X_new, family=sm.families.Poisson(), offset=np.log(exp)).fit()\n",
    "    LR      = 2 * (res_new.llf - base_llf)\n",
    "    df_diff = res_new.df_model - base_res.df_model\n",
    "    p       = chi2.sf(LR, df_diff)\n",
    "    tests.append({\n",
    "        'interaction_set': label,\n",
    "        'k': int(res_new.df_model + 1),\n",
    "        'AIC': float(res_new.aic),\n",
    "        'ΔAIC': float(res_new.aic - base_aic),\n",
    "        'LR_stat': float(LR),\n",
    "        'df_diff': int(df_diff),\n",
    "        'p_value': float(p)\n",
    "    })\n",
    "    return res_new\n",
    "\n",
    "# Individually present interactions (guarded by parents)\n",
    "which_individual = []\n",
    "if INCLUDE_BC: which_individual.append({'bcXyear'})\n",
    "if INCLUDE_DW: which_individual.append({'dwXyear'})\n",
    "which_individual.append({'areaXyear'})  # always check area stability\n",
    "\n",
    "fitted_variants = {}\n",
    "for w in which_individual:\n",
    "    X_tmp = add_time_interactions(X_base, df_hist, which=w)\n",
    "    key = next(iter(w))\n",
    "    fitted_variants[key] = fit_and_compare(X_tmp, key)\n",
    "\n",
    "# ALL present\n",
    "which_all = set()\n",
    "if INCLUDE_BC: which_all.add('bcXyear')\n",
    "if INCLUDE_DW: which_all.add('dwXyear')\n",
    "which_all.add('areaXyear')\n",
    "X_all = add_time_interactions(X_base, df_hist, which=which_all)\n",
    "res_time = fit_and_compare(X_all, 'ALL')\n",
    "\n",
    "time_val_df = pd.DataFrame(tests).sort_values('AIC')\n",
    "print(\"\\nTemporal interaction comparison vs BASE:\")\n",
    "print(time_val_df.to_string(index=False))\n",
    "\n",
    "# Model-validation plots\n",
    "df_pred = df_hist.copy()\n",
    "df_pred['mu_hat'] = res_time.predict(X_all, offset=np.log(df_pred['exposure']))\n",
    "\n",
    "def summarize_factor(df, factor):\n",
    "    g = df.groupby([factor, 'year'], as_index=False).agg(\n",
    "        mu_hat_sum=('mu_hat','sum'),\n",
    "        exposure_sum=('exposure','sum')\n",
    "    )\n",
    "    g['freq_hat'] = g['mu_hat_sum'] / g['exposure_sum']  # rate\n",
    "    return g\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 3), sharey=True)\n",
    "\n",
    "for ax, factor in zip(axes, [\"building_class\", \"dwelling_type\", \"area_band\"]):\n",
    "    df_fac = summarize_factor(df_pred, factor)\n",
    "\n",
    "    # enforce ordered categories so seaborn doesn't reorder\n",
    "    if factor == \"building_class\":\n",
    "        order = [\"1\", \"2\", \"3\"]\n",
    "        df_fac[factor] = df_fac[factor].astype(str)\n",
    "    elif factor == \"dwelling_type\":\n",
    "        order = [\"BR\", \"HR\"]\n",
    "    else:\n",
    "        if pd.api.types.is_categorical_dtype(df_fac[factor]):\n",
    "            order = list(df_fac[factor].cat.categories)\n",
    "        else:\n",
    "            order = list(pd.unique(df_fac[factor]))\n",
    "    df_fac[factor] = pd.Categorical(df_fac[factor], categories=order, ordered=True)\n",
    "\n",
    "    #  left axis: log(expected frequency)\n",
    "    sns.lineplot(\n",
    "        data=df_fac,\n",
    "        x=factor, y=np.log(df_fac['freq_hat']),\n",
    "        hue='year', marker='o', ax=ax, palette='tab10',\n",
    "        sort=False, estimator=None, legend=(ax is axes[0])\n",
    "    )\n",
    "    ax.set_xlabel(factor.replace('_',' ').title())\n",
    "    ax.set_ylabel('log(Expected Frequency)')\n",
    "    ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax.set_title(factor.replace('_',' ').title())\n",
    "    # clamp x so lines cannot extend beyond categories\n",
    "    ax.set_xlim(-0.5, len(order) - 0.5)\n",
    "\n",
    "    # right axis: exposure bars in same order\n",
    "    ax2 = ax.twinx()\n",
    "    sns.barplot(\n",
    "        data=df_fac, x=factor, y='exposure_sum',\n",
    "        order=order, ax=ax2, color='grey', alpha=0.2, errorbar=None\n",
    "    )\n",
    "    ax2.set_ylabel('Exposure (sum)')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "# shared legend (Year)\n",
    "leg = axes[0].legend(title='Year', loc='upper left', framealpha=0.35, fontsize=8)\n",
    "if leg is not None:\n",
    "    leg.get_frame().set_edgecolor('none')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Residuals Validation\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(choice_res.resid_deviance, bins=40, edgecolor='black', alpha=0.7)\n",
    "plt.title(f\"Deviance residuals — {choice['spec']} ({choice['family']})\")\n",
    "plt.xlabel(\"Deviance residual\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "mu = choice_res.fittedvalues\n",
    "resid = choice_res.resid_deviance\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(mu, resid, alpha=0.4)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Fitted mean (μ̂)\")\n",
    "plt.ylabel(\"Deviance residuals\")\n",
    "plt.title(f\"Residuals vs Fitted — {choice['spec']}\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65056062",
   "metadata": {},
   "source": [
    "### Insight 2\n",
    "Predicted log-frequencies remain stable across 2020–2024, confirming that the bc×dw interaction captures structural differences without temporal drift.\n",
    "- Building class: Frequency decreases with class level (1→3), consistent across years; exposure concentrated in class 1.\n",
    "- Dwelling type: HR shows slightly lower frequency than BR; stable pattern and larger exposure share.\n",
    "- Area band: Frequency rises with area; parallel slopes across years indicate no time effect.\n",
    "\n",
    "Residual diagnostics\n",
    "Residuals show no trend or heteroscedasticity. Deviance residuals are centered near 0 with mild variance growth at higher fitted means, consistent with Poisson assumptions.\n",
    "\n",
    "Conclusion: The Poisson GLM (area=banded | bc=Y | dw=Y | inter=bc×dw) is statistically sound and temporally stable, suitable for pricing segmentation without evidence of misspecification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend and calibration sanity check \n",
    "year_trend = (\n",
    "    df_hist\n",
    "    .assign(pred=choice_res.mu)\n",
    "    .groupby(\"year\", observed=True)\n",
    "    .agg(\n",
    "        obs_claims=(\"claims_number\", \"sum\"),\n",
    "        exp=(\"exposure\", \"sum\"),\n",
    "        pred_claims=(\"pred\", \"sum\")\n",
    "    )\n",
    "    .assign(\n",
    "        obs_freq=lambda d: d[\"obs_claims\"] / d[\"exp\"],\n",
    "        pred_freq=lambda d: d[\"pred_claims\"] / d[\"exp\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Exposure-weighted frequency trend by year:\")\n",
    "print(year_trend[[\"obs_freq\", \"pred_freq\"]].round(6))\n",
    "\n",
    "# Plot observed vs predicted frequency trend\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(year_trend.index, year_trend[\"obs_freq\"], marker=\"o\", label=\"Observed frequency\", color=\"black\")\n",
    "plt.plot(year_trend.index, year_trend[\"pred_freq\"], marker=\"o\", label=\"Predicted frequency\", color=\"steelblue\")\n",
    "plt.xticks(year_trend.index.astype(int))\n",
    "plt.title(\"Trend and calibration sanity check\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Frequency (claims / exposure)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend(title=\"Source\", framealpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf617b3",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Check indicates a year trend is ignored in the current model. Since time interaction with area_band was the only significantly improving coefficient (~4 ΔAIC), an area_bandXyear interaction is included in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25838775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FINAL MODEL with numeric time slope =====\n",
    "# final non-time spec here:\n",
    "USE_BANDED_AREA = True \n",
    "INCLUDE_BC = True\n",
    "INCLUDE_DW = True\n",
    "INTERACTIONS = {'bcXdw', 'bcXarea'}\n",
    "\n",
    "# Build base (non-time) design for TRAIN (2020–2024)\n",
    "X_base = make_X(\n",
    "    df_hist,\n",
    "    use_area_band=USE_BANDED_AREA,\n",
    "    include_bc=INCLUDE_BC,\n",
    "    include_dw=INCLUDE_DW,\n",
    "    interactions=INTERACTIONS\n",
    ")\n",
    "\n",
    "# Add numeric time slope(s): t and (optionally) area×t\n",
    "# Set include_areaXt=False if you only want a plain time slope.\n",
    "X_final, BASE_YEAR = add_time_trend(\n",
    "    X_base, df_hist,\n",
    "    include_t=True,\n",
    "    include_areaXt=True,\n",
    "    area_banded=USE_BANDED_AREA\n",
    ")\n",
    "\n",
    "# Fit Poisson with offset (as before)\n",
    "y   = df_hist[\"claims_number\"]\n",
    "exp = df_hist[\"exposure\"]\n",
    "\n",
    "final_model = sm.GLM(y, X_final, family=sm.families.Poisson(), offset=np.log(exp)).fit()\n",
    "\n",
    "print(\"\\n=== FINAL TIME-TREND MODEL SUMMARY ===\")\n",
    "print(final_model.summary())\n",
    "print(f\"\\nAnchored base_year used for t: {BASE_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c197aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model valdiation (final time-calibrated model)\n",
    "df_pred = df_hist.copy()\n",
    "df_pred['mu_hat'] = final_model.predict(X_final, offset=np.log(df_pred['exposure']))\n",
    "\n",
    "def summarize_factor(df, factor):\n",
    "    g = df.groupby([factor, 'year'], as_index=False, observed=True).agg(\n",
    "        mu_hat_sum=('mu_hat', 'sum'),\n",
    "        exposure_sum=('exposure', 'sum')\n",
    "    )\n",
    "    g['freq_hat'] = g['mu_hat_sum'] / g['exposure_sum']\n",
    "    return g\n",
    "\n",
    "factors = ['building_class', 'dwelling_type', 'area_band']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 3), sharey=True)\n",
    "\n",
    "for ax, factor in zip(axes, factors):\n",
    "    df_fac = summarize_factor(df_pred, factor)\n",
    "    if factor == 'building_class':\n",
    "        order = ['1', '2', '3']\n",
    "        df_fac[factor] = df_fac[factor].astype(str)\n",
    "    elif factor == 'dwelling_type':\n",
    "        order = ['BR', 'HR']\n",
    "    else:\n",
    "        order = [str(c) for c in pd.unique(df_fac[factor])]\n",
    "\n",
    "    df_fac[factor] = pd.Categorical(df_fac[factor], categories=order, ordered=True)\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df_fac,\n",
    "        x=factor, y=np.log(df_fac['freq_hat']),\n",
    "        hue='year', marker='o', ax=ax, palette='tab10',\n",
    "        sort=False, estimator=None, legend=(ax is axes[0])\n",
    "    )\n",
    "    ax.set_xlim(-0.5, len(order) - 0.5)\n",
    "    ax.set_xlabel(factor.replace('_', ' ').title())\n",
    "    ax.set_ylabel('log(Expected Frequency)')\n",
    "    ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax.set_title(factor.replace('_', ' ').title())\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    sns.barplot(data=df_fac, x=factor, y='exposure_sum',\n",
    "                order=order, ax=ax2, color='grey', alpha=0.2, errorbar=None)\n",
    "    ax2.set_ylabel('Exposure (sum)')\n",
    "    ax2.set_yscale('log')\n",
    "\n",
    "leg = axes[0].legend(title='Year', loc='upper left', framealpha=0.35, fontsize=8)\n",
    "if leg: leg.get_frame().set_edgecolor('none')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio-level check \n",
    "obs_freq = df_hist['claims_number'].sum() / df_hist['exposure'].sum()\n",
    "pred_freq = final_model.mu.sum() / df_hist['exposure'].sum()\n",
    "\n",
    "print(f\"Observed portfolio frequency: {obs_freq:.4f}\")\n",
    "print(f\"Predicted portfolio frequency: {pred_freq:.4f}\")\n",
    "print(f\"Deviation: {(pred_freq/obs_freq - 1)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment-level checks\n",
    "for var in ['building_class', 'dwelling_type', 'area_band']:\n",
    "    check = (\n",
    "        df_hist\n",
    "        .assign(pred=final_model.mu)\n",
    "        .groupby(var)\n",
    "        .agg(obs_freq=('claims_number', lambda x: x.sum()),\n",
    "             exp=('exposure', 'sum'),\n",
    "             pred_freq=('pred', 'sum'))\n",
    "        .assign(obs_rate=lambda d: d['obs_freq']/d['exp'],\n",
    "                pred_rate=lambda d: d['pred_freq']/d['exp'],\n",
    "                deviation=lambda d: (d['pred_rate']/d['obs_rate'] - 1)*100)\n",
    "    )\n",
    "    print(f\"\\n--- {var} ---\")\n",
    "    print(check[['obs_rate','pred_rate','deviation']].round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d32726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend and calibration sanity check\n",
    "trend = df_hist.copy()\n",
    "trend['pred_freq'] = final_model.predict(X_final, offset=np.log(trend['exposure'])) / trend['exposure']\n",
    "\n",
    "trend_check = (\n",
    "    trend.groupby('year', as_index=False)\n",
    "         .agg(obs_freq=('claims_number', lambda x: x.sum() / x.count()),\n",
    "              pred_freq=('pred_freq', 'mean'))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(trend_check['year'], trend_check['obs_freq'], 'o-', label='Observed frequency', color='black')\n",
    "plt.plot(trend_check['year'], trend_check['pred_freq'], 'o-', label='Predicted frequency', color='steelblue')\n",
    "plt.xticks(year_trend.index.astype(int))\n",
    "plt.title('Trend and calibration sanity check')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency (claims / exposure)')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend(title='Source', framealpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb10ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save key model specs to disk (includes time-trend info)\n",
    "res = final_model\n",
    "\n",
    "frozen_model = {\n",
    "    \"spec\": {\n",
    "        \"use_area_band\": USE_BANDED_AREA,\n",
    "        \"include_bc\": INCLUDE_BC,\n",
    "        \"include_dw\": INCLUDE_DW,\n",
    "        \"interactions\": sorted(list(INTERACTIONS)),\n",
    "        \"time\": {\n",
    "            \"base_year\": int(BASE_YEAR),\n",
    "            \"include_t\": True,\n",
    "            \"include_areaXt\": True\n",
    "        }\n",
    "    },\n",
    "    \"family\": \"Poisson\",\n",
    "    \"aic\": float(res.aic),\n",
    "    \"bic\": float(getattr(res, \"bic\", (np.log(res.nobs) * len(res.params) - 2.0 * res.llf))),\n",
    "    \"pearson_over_df\": float(res.pearson_chi2 / res.df_resid),\n",
    "    \"k_params\": int(len(res.params)),\n",
    "    \"params\": {k: float(v) for k, v in res.params.items()},\n",
    "    \"exog_names\": list(res.model.exog_names),\n",
    "}\n",
    "\n",
    "out = \"final_model_spec.json\"\n",
    "import json\n",
    "with open(out, \"w\") as f:\n",
    "    json.dump(frozen_model, f, indent=2)\n",
    "print(f\"Saved: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecefbe3",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab16a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict 2025 expected claims using the frozen spec\n",
    "assert (df_2025['exposure'] > 0).all(), \"2025 contains non-positive exposure; cannot use log-offset.\"\n",
    "\n",
    "# 2) Rebuild 2025 base X with the same mains & interactions\n",
    "use_area_band = frozen_model[\"spec\"][\"use_area_band\"]\n",
    "include_bc    = frozen_model[\"spec\"][\"include_bc\"]\n",
    "include_dw    = frozen_model[\"spec\"][\"include_dw\"]\n",
    "interactions  = set(frozen_model[\"spec\"][\"interactions\"])\n",
    "\n",
    "X_2025_base = make_X(\n",
    "    df_2025,\n",
    "    use_area_band=use_area_band,\n",
    "    include_bc=include_bc,\n",
    "    include_dw=include_dw,\n",
    "    interactions=interactions\n",
    ")\n",
    "\n",
    "# 3) Add the same time slopes with the same base_year and flags\n",
    "time_cfg = frozen_model[\"spec\"][\"time\"]\n",
    "X_2025, _ = add_time_trend(\n",
    "    X_2025_base, df_2025,\n",
    "    base_year=time_cfg[\"base_year\"],\n",
    "    include_t=time_cfg[\"include_t\"],\n",
    "    include_areaXt=time_cfg[\"include_areaXt\"],\n",
    "    area_banded=use_area_band\n",
    ")\n",
    "\n",
    "# 4) Align columns and predict μ̂\n",
    "exog_names = frozen_model[\"exog_names\"]\n",
    "X_2025 = X_2025.reindex(columns=exog_names, fill_value=0.0)\n",
    "\n",
    "df_2025['mu_hat'] = final_model.predict(X_2025, offset=np.log(df_2025['exposure']))\n",
    "\n",
    "print(\"Sample 2025 predictions:\")\n",
    "print(df_2025[['policy_id','building_class','dwelling_type','area','year','exposure','mu_hat']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa516840",
   "metadata": {},
   "outputs": [],
   "source": [
    "## === Case Questions & Answers ===\n",
    "\n",
    "# --- Q1: Total expected claim cost and premium at 70% LR ---\n",
    "PB_2025   = 58_800   # SEK per claim\n",
    "TARGET_LR = 0.70\n",
    "\n",
    "df_2025[\"expected_claim_cost\"] = df_2025[\"mu_hat\"] * PB_2025\n",
    "df_2025[\"premium_proposal\"]    = df_2025[\"expected_claim_cost\"] / TARGET_LR\n",
    "\n",
    "total_expected_cost  = df_2025[\"expected_claim_cost\"].sum()\n",
    "total_premium_needed = total_expected_cost / TARGET_LR\n",
    "\n",
    "print(\"\\nQ1: Total expected claim cost and premium at 70% LR\")\n",
    "print(f\"Total expected claim cost (2025): {total_expected_cost:,.0f} SEK\")\n",
    "print(f\"Total premium for LR={TARGET_LR:.0%}: {total_premium_needed:,.0f} SEK\")\n",
    "\n",
    "# --- Q2: Highest-premium policy ---\n",
    "cols_show = ['policy_id','building_class','dwelling_type','area','year','exposure','mu_hat','expected_claim_cost','premium_proposal']\n",
    "top = df_2025.loc[df_2025['premium_proposal'].idxmax(), cols_show]\n",
    "print(\"\\nQ2: Highest-premium policy (labeled):\")\n",
    "for k, v in top.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# --- Q3: Estimators confidence levels ---\n",
    "print('\\nQ3: Estimators confidence levels\\n')\n",
    "cov = final_model.cov_params()\n",
    "X   = X_2025.values\n",
    "\n",
    "# Var(η̂) = X Σ Xᵀ\n",
    "var_eta = np.einsum('ij,jk,ik->i', X, cov, X)\n",
    "se_eta  = np.sqrt(np.maximum(var_eta, 0.0))  # guard tiny negatives\n",
    "\n",
    "# Delta method to μ̂: Var(μ̂) ≈ (dμ/dη)^2 Var(η) = μ̂^2 Var(η)\n",
    "mu_hat = df_2025['mu_hat'].values\n",
    "se_mu  = mu_hat * se_eta\n",
    "cv_mu  = np.divide(se_mu, mu_hat, out=np.full_like(se_mu, np.nan), where=mu_hat>0)\n",
    "\n",
    "df_2025['se_eta'] = se_eta\n",
    "df_2025['se_mu']  = se_mu\n",
    "df_2025['cv_mu']  = cv_mu   # coefficient of variation on μ̂\n",
    "\n",
    "cols = ['policy_id','building_class','dwelling_type','area','year','exposure','mu_hat','se_eta','se_mu','cv_mu']\n",
    "print(df_2025[cols].head(10))\n",
    "\n",
    "print(\"\\nMost certain (lowest cv_mu among μ̂>0):\")\n",
    "mask = df_2025['mu_hat'] > 0\n",
    "print(df_2025[mask].nsmallest(10, 'cv_mu')[cols])\n",
    "\n",
    "print(\"\\nLeast certain (highest cv_mu among μ̂>0):\")\n",
    "print(df_2025[mask].nlargest(10, 'cv_mu')[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4317063",
   "metadata": {},
   "source": [
    "# Case Q&A\n",
    "1)  Based on the selected Poisson GLM model (area_banded + bc + dw main effects + bc*area + area_banded*year interactions), the expected claim cost for 2025 would be ~85.8 mSEK. Moreover, based on the Prisbasbelopp 2025 at 58'800 SEK per claim and the target LR at 70%, the minimum total premium required would be ~122.5 mSEK. These figures imply an average technical premium about 43 % above expected claims, in line with the target profitability level.These results assume the exposure and portfolio composition in 2025 are consistent with historical data. Any significant change would affect those projected estimates.\n",
    "\n",
    "2) The highest premium corresponds to a Building Class 1, Brick (BR) dwelling of area ≈ 2 650 m².\n",
    "Its predicted claim frequency is around 0.082 claims / exposure, leading to an expected claim cost ≈ 4.8 kSEK and a technical premium ≈ 6.9 kSEK at the 70 % LR. This profile lies among the largest and most exposed risks in the dataset—hence the higher expected loss and premium.\n",
    "\n",
    "3) Uncertainty was quantified using the GLM covariance matrix:  \n",
    "    Var(η̂)=X Σ Xᵀ, with the delta method giving the standard error and coefficient of variation (cv = se/μ̂).\n",
    "    - Most certain predictions (cv ≈ 6 %) come from Building Class 1 – HR dwellings with medium-to-large areas, well-represented in the training data.\n",
    "    - Least certain (cv ≈ 19 %) appear for small Building Class 3 – HR dwellings, a relatively rare and low-exposure segment.\n",
    "\n",
    "In short, the model is most confident in frequent, data-rich profiles and least confident in sparse, low-area risks—a pattern typical of portfolio GLMs. High-uncertainty cells could later receive credibility adjustments or pooling in tariff refinement. For high pred_se (or relative uncertainty) → this policy’s premium is less credible, likely because it sits in a sparse region of the data (rare combination of factors). For low pred_se → prediction is stable, many similar observations support it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (.venv311)",
   "language": "python",
   "name": ".venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
